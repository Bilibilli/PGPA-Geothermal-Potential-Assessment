{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from scipy.stats import truncnorm, beta, norm, lognorm, triang\n",
    "import os\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================= 1. 参数设置 =======================\n",
    "# --- 输入文件 ---\n",
    "excel_path = r\"G:\\Remote sensing paper\\RESAMPLE\\new revise\\Data.xlsx\"\n",
    "\n",
    "# --- 输出文件 ---\n",
    "output_dir = r\"G:\\Remote sensing paper\\RESAMPLE\\new revise\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_iter_excel_base = os.path.join(output_dir, \"iterations_results.xlsx\") \n",
    "output_geotiff = os.path.join(output_dir, \"geothermal_potential_mean.tif\")\n",
    "sobol_results_excel = os.path.join(output_dir, \"sobol_sensitivity_results_high_sample.xlsx\")\n",
    "sobol_plot_path = os.path.join(output_dir, \"sobol_sensitivity_plot_high_sample.png\")\n",
    "\n",
    "# --- 模拟参数 ---\n",
    "n_iterations = 10000  # 蒙特卡洛模拟的迭代次数\n",
    "batch_size = 1000     # 每批保存的迭代次数\n",
    "sobol_n_samples = 4096 # Sobol分析的样本量\n",
    "\n",
    "# --- 指标列名映射 ---\n",
    "indicator_columns = {\n",
    "    \"A1\": \"A1_rock\", \"A2\": \"A2_fault\", \"A3\": \"A3_depth\",\n",
    "    \"A4\": \"A4_temp\", \"A5\": \"A5_heatflow\", \"A6\": \"A6_mag\",\n",
    "    \"A7\": \"A7_land\"\n",
    "}\n",
    "\n",
    "# --- 权重参数 ---\n",
    "triang_params = {\n",
    "    \"A1\": (0.1489, 0.1530, 0.1532), \"A2\": (0.2124, 0.2140, 0.2182),\n",
    "    \"A3\": (0.0879, 0.0917, 0.1022), \"A4\": (0.0541, 0.0582, 0.0603),\n",
    "    \"A5\": (0.2853, 0.3212, 0.3344), \"A6\": (0.0946, 0.1012, 0.1155),\n",
    "    \"A7\": (0.0607, 0.0636, 0.0695),\n",
    "}\n",
    "indicator_order = list(triang_params.keys()) \n",
    "\n",
    "# --- Sobol分析参数范围 ---\n",
    "sobol_problem = {\n",
    "    'num_vars': 14,\n",
    "    'names': [\n",
    "        'w_A1', 'w_A2', 'w_A3', 'w_A4', 'w_A5', 'w_A6', 'w_A7', # 权重参数\n",
    "        'd_A1', 'd_A2', 'd_A3', 'd_A4', 'd_A5', 'd_A6', 'd_A7'  # 数据不确定性参数\n",
    "    ],\n",
    "    'bounds': [\n",
    "        # 权重范围\n",
    "        [0.1489, 0.1532], [0.2124, 0.2182], [0.0879, 0.1022], [0.0541, 0.0603],\n",
    "        [0.2853, 0.3344], [0.0946, 0.1155], [0.0607, 0.0695],\n",
    "        # 数据不确定性范围 (简化为[0,1]的因子)\n",
    "        [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0],\n",
    "        [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_crs = \"EPSG:4326\"\n",
    "\n",
    "# ======================= 2. 向量化采样函数定义  =======================\n",
    "rock_params = {\n",
    "    1: {\"λ_mean\": 2.80, \"λ_std\": 0.50, \"k_mean\": 3.00e-04, \"k_std\": 1.5e-04, \"norm\": 0.1},\n",
    "    2: {\"λ_mean\": 2.70, \"λ_std\": 0.25, \"k_mean\": 1.65e-03, \"k_std\": 6.75e-04, \"norm\": 0.01},\n",
    "    3: {\"λ_mean\": 1.90, \"λ_std\": 0.25, \"k_mean\": 1.00e-07, \"k_std\": 5.00e-08, \"norm\": 0.001},\n",
    "    4: {\"λ_mean\": 3.00, \"λ_std\": 0.65, \"k_mean\": 3.00e-04, \"k_std\": 1.50e-04, \"norm\": 0.1},\n",
    "    5: {\"λ_mean\": 2.20, \"λ_std\": 0.50, \"k_mean\": 5.01e-07, \"k_std\": 2.50e-07, \"norm\": 0.001},\n",
    "    6: {\"λ_mean\": 3.15, \"λ_std\": 0.525,\"k_mean\": 1.50e-08, \"k_std\": 7.50e-09, \"norm\": 0.001},\n",
    "    7: {\"λ_mean\": 0.94, \"λ_std\": 0.085,\"k_mean\": 1.00e-03, \"k_std\": 5.00e-04, \"norm\": 0.01},\n",
    "}\n",
    "\n",
    "def truncated_normal_vec(mean, std, low=0, high=np.inf, size=1):\n",
    "    if isinstance(mean, (int, float)): mean = np.full(size, mean)\n",
    "    if isinstance(std, (int, float)): std = np.full(size, std)\n",
    "    \n",
    "    # 防止std为0导致除法错误\n",
    "    std[std <= 0] = 1e-9\n",
    "    \n",
    "    a, b = (low - mean) / std, (high - mean) / std\n",
    "    return truncnorm.rvs(a, b, loc=mean, scale=std, size=size)\n",
    "\n",
    "def sample_A1_vec(rock_col):\n",
    "    results = np.zeros_like(rock_col, dtype=float)\n",
    "    for rock_type, params in rock_params.items():\n",
    "        mask = (rock_col == rock_type)\n",
    "        count = np.sum(mask)\n",
    "        if count == 0: continue\n",
    "        \n",
    "        λ = truncated_normal_vec(params[\"λ_mean\"], params[\"λ_std\"], size=count)\n",
    "        k = truncated_normal_vec(params[\"k_mean\"], params[\"k_std\"], size=count)\n",
    "        \n",
    "        value = λ * k * 1e3\n",
    "        results[mask] = value / params[\"norm\"]\n",
    "    return np.clip(results, 0, 1)\n",
    "\n",
    "def sample_A2_vec(fault_col):\n",
    "    size = len(fault_col)\n",
    "    normalized_value = fault_col / 99.0\n",
    "    rand_mask = np.random.rand(size) < 0.6\n",
    "    results = beta.rvs(2, 5, size=size) * normalized_value\n",
    "    results[rand_mask] = 0\n",
    "    return np.clip(results, 0, 1)\n",
    "\n",
    "def sample_A3_vec(depth_col):\n",
    "    depth_m = (depth_col / 17.0) * 4000\n",
    "    sampled = norm.rvs(depth_m, 100)\n",
    "    return np.clip(1 - sampled / 4000, 0, 1)\n",
    "\n",
    "def sample_A4_vec(temp_col):\n",
    "    temp_c = (temp_col - 1) / 9.0 * 40\n",
    "    sampled = norm.rvs(temp_c, 2)\n",
    "    return np.clip(sampled / 40, 0, 1)\n",
    "\n",
    "def sample_A5_vec(heatflow_col):\n",
    "    size = len(heatflow_col)\n",
    "    results = np.zeros(size, dtype=float)\n",
    "    \n",
    "    neg_mask = heatflow_col < 0\n",
    "    pos_mask = ~neg_mask\n",
    "    \n",
    "    results[neg_mask] = np.random.choice([30, 40, 50, 60, 70, 80], size=np.sum(neg_mask)) / 100.0\n",
    "    results[pos_mask] = norm.rvs(heatflow_col[pos_mask], 5) / 100.0\n",
    "    return np.clip(results, 0, 1)\n",
    "\n",
    "def sample_A6_vec(mag_col):\n",
    "    normalized_value = (mag_col + 209) / 474.0\n",
    "    lognorm_adjusted = lognorm.rvs(0.5, scale=normalized_value * 100)\n",
    "    return np.clip(lognorm_adjusted / 100.0, 0, 1)\n",
    "\n",
    "def sample_A7_vec(land_col):\n",
    "    lulc_weights = {1: 0.3, 2: 0.2, 4: 0.4, 5: 0.5, 6: 0.1, 7: 0.1, 9: 0.2, 10: 0.9}\n",
    "    base_values = land_col.map(lambda x: lulc_weights.get(x, 0.5)).to_numpy()\n",
    "    sampled = np.random.normal(base_values, 0.1)\n",
    "    return np.clip(sampled, 0, 1)\n",
    "\n",
    "sampling_functions_vec = {\n",
    "    \"A1\": sample_A1_vec, \"A2\": sample_A2_vec, \"A3\": sample_A3_vec, \"A4\": sample_A4_vec,\n",
    "    \"A5\": sample_A5_vec, \"A6\": sample_A6_vec, \"A7\": sample_A7_vec,\n",
    "}\n",
    "\n",
    "# ======================= 3. 蒙特卡洛模拟 =======================\n",
    "\n",
    "print(\"1. 读取Excel数据...\")\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    # 将原始数据列转换为NumPy数组以便快速访问\n",
    "    raw_data = {ind: df[col].to_numpy() for ind, col in indicator_columns.items()}\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误：找不到文件 {excel_path}。请检查路径。\")\n",
    "    exit()\n",
    "\n",
    "required_cols = [\"X\", \"Y\"] + list(indicator_columns.values())\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        print(f\"错误：Excel文件中缺少必需的列: '{col}'\")\n",
    "        exit()\n",
    "\n",
    "print(f\"   - 数据读取成功，共 {len(df)} 个点。\")\n",
    "\n",
    "# 初始化用于存储所有迭代结果的数组\n",
    "results_matrix = np.zeros((len(df), n_iterations))\n",
    "\n",
    "print(f\"\\n2. 开始执行 {n_iterations} 次蒙特卡洛模拟 (已优化)...\")\n",
    "for i in range(n_iterations):\n",
    "    # --- a. 采样权重 ---\n",
    "    sampled_weights_list = []\n",
    "    for indicator in indicator_order:\n",
    "        low, mode, high = triang_params[indicator]\n",
    "        c = (mode - low) / (high - low) if (high - low) > 0 else 0.5\n",
    "        sampled_weights_list.append(triang.rvs(c, loc=low, scale=high - low))\n",
    "    \n",
    "    weights = np.array(sampled_weights_list)\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # --- b. 向量化采样有利度得分 ---\n",
    "    sampled_favorability = np.zeros((len(df), len(indicator_order)))\n",
    "    for j, indicator in enumerate(indicator_order):\n",
    "        sampler_func = sampling_functions_vec[indicator]\n",
    "        if indicator == 'A7': \n",
    "             sampled_favorability[:, j] = sampler_func(df[indicator_columns[indicator]])\n",
    "        else:\n",
    "             sampled_favorability[:, j] = sampler_func(raw_data[indicator])\n",
    "    \n",
    "    # --- c. 计算本次迭代的加权综合得分 ---\n",
    "    iteration_result = np.dot(sampled_favorability, weights)\n",
    "    results_matrix[:, i] = iteration_result\n",
    "    \n",
    "    # 显示进度\n",
    "    if (i + 1) % 500 == 0 or (i + 1) == n_iterations:\n",
    "        print(f\"   - 完成进度: {i+1}/{n_iterations} ({((i + 1) / n_iterations) * 100:.1f}%)\")\n",
    "\n",
    "print(\"   - 模拟完成。\")\n",
    "\n",
    "# ======================= 4. 结果保存 =======================\n",
    "\n",
    "print(\"\\n3. 分批保存迭代结果到Excel文件...\")\n",
    "n_batches = (n_iterations + batch_size - 1) // batch_size\n",
    "for batch in range(n_batches):\n",
    "    start_idx = batch * batch_size\n",
    "    end_idx = min((batch + 1) * batch_size, n_iterations)\n",
    "    \n",
    "    batch_iter_cols = [f\"iter_{i+1}\" for i in range(start_idx, end_idx)]\n",
    "    batch_df = pd.DataFrame(results_matrix[:, start_idx:end_idx], columns=batch_iter_cols)\n",
    "    \n",
    "    batch_output_df = pd.concat([df[[\"X\", \"Y\"]], batch_df], axis=1)\n",
    "    \n",
    "    batch_filename = output_iter_excel_base.replace('.xlsx', f'_batch_{batch+1}_of_{n_batches}.xlsx')\n",
    "    batch_output_df.to_excel(batch_filename, index=False)\n",
    "    print(f\"   - 批次 {batch+1}/{n_batches} 已保存: {batch_filename}\")\n",
    "print(\"   - 所有批次保存完成。\")\n",
    "\n",
    "\n",
    "print(\"\\n4. 计算平均值并生成GeoTIFF和汇总文件...\")\n",
    "mean_probability = results_matrix.mean(axis=1)\n",
    "df[\"probability_mean\"] = mean_probability\n",
    "\n",
    "# 生成栅格文件\n",
    "x_unique = np.sort(df[\"X\"].unique())\n",
    "y_unique = np.sort(df[\"Y\"].unique())[::-1]\n",
    "pixel_size_x = np.mean(np.diff(x_unique)) if len(x_unique) > 1 else 1.0\n",
    "pixel_size_y = -np.mean(np.diff(y_unique)) if len(y_unique) > 1 else -1.0 \n",
    "width = len(x_unique)\n",
    "height = len(y_unique)\n",
    "# from_origin(x_corner, y_corner, x_size, y_size)\n",
    "transform = from_origin(x_unique[0] - pixel_size_x / 2, y_unique[0] - pixel_size_y / 2, pixel_size_x, -pixel_size_y)\n",
    "\n",
    "grid = np.full((height, width), np.nan, dtype='float32')\n",
    "df['row_idx'] = df['Y'].apply(lambda y: np.where(y_unique == y)[0][0])\n",
    "df['col_idx'] = df['X'].apply(lambda x: np.where(x_unique == x)[0][0])\n",
    "grid[df['row_idx'], df['col_idx']] = df['probability_mean']\n",
    "\n",
    "meta = {'driver': 'GTiff', 'height': height, 'width': width, 'count': 1, 'dtype': 'float32',\n",
    "        'crs': output_crs, 'transform': transform, 'nodata': np.nan}\n",
    "with rasterio.open(output_geotiff, 'w', **meta) as dst:\n",
    "    dst.write(grid, 1)\n",
    "print(f\"   - 平均概率栅格图已保存到: {output_geotiff}\")\n",
    "\n",
    "summary_df = df[[\"X\", \"Y\", \"probability_mean\"]]\n",
    "summary_filename = output_iter_excel_base.replace('.xlsx', '_summary.xlsx')\n",
    "summary_df.to_excel(summary_filename, index=False)\n",
    "print(f\"   - 汇总文件已保存: {summary_filename}\")\n",
    "\n",
    "# ======================= 5. Sobol 敏感性分析 =======================\n",
    "print(\"\\n5. 开始执行 Sobol 敏感性分析...\")\n",
    "\n",
    "# --- a. 准备基准有利度数据 ---\n",
    "base_favorability_from_mc = np.zeros((len(df), len(indicator_order)), dtype=np.float32)\n",
    "for j, indicator in enumerate(indicator_order):\n",
    "    sampler_func = sampling_functions_vec[indicator]\n",
    "    if indicator == 'A7':\n",
    "        base_favorability_from_mc[:, j] = sampler_func(df[indicator_columns[indicator]])\n",
    "    else:\n",
    "        base_favorability_from_mc[:, j] = sampler_func(raw_data[indicator])\n",
    "print(\"   - Sobol分析的基准有利度数据已准备。\")\n",
    "\n",
    "\n",
    "# --- b. 定义Sobol模型函数  ---\n",
    "def sobol_model_batch(params_batch):\n",
    "    # params_batch 是一个 (batch_size, D) 的数组, D是参数个数 (14)\n",
    "    weights_samples = params_batch[:, :7]\n",
    "    data_uncertainty_samples = params_batch[:, 7:]\n",
    "    \n",
    "    # 归一化权重\n",
    "    weights_samples /= np.sum(weights_samples, axis=1, keepdims=True)\n",
    "    perturbed_favorability = (data_uncertainty_samples[:, np.newaxis, :] * base_favorability_from_mc[np.newaxis, :, :] +\n",
    "                              (1 - data_uncertainty_samples[:, np.newaxis, :]) * (1 - base_favorability_from_mc[np.newaxis, :, :]))\n",
    "    \n",
    "    results_per_point = np.einsum('ij,ikj->ik', weights_samples, perturbed_favorability)\n",
    "    return np.mean(results_per_point, axis=1)\n",
    "\n",
    "\n",
    "# --- c. 生成参数样本 ---\n",
    "param_values = saltelli.sample(sobol_problem, sobol_n_samples, calc_second_order=True)\n",
    "print(f\"   - Saltelli 样本已生成，总运行次数: {len(param_values)}\")\n",
    "\n",
    "# --- d. 分批次运行模型  ---\n",
    "# 初始化输出数组\n",
    "Y = np.zeros(param_values.shape[0])\n",
    "# 定义批处理大小\n",
    "sobol_batch_size = 1024  \n",
    "num_batches = (len(param_values) + sobol_batch_size - 1) // sobol_batch_size\n",
    "\n",
    "print(\"   - 开始分批次运行Sobol模型...\")\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * sobol_batch_size\n",
    "    end_idx = min(start_idx + sobol_batch_size, len(param_values))\n",
    "    \n",
    "    # 获取当前批次的参数\n",
    "    params_batch = param_values[start_idx:end_idx]\n",
    "    \n",
    "    # 对当前批次运行模型\n",
    "    Y[start_idx:end_idx] = sobol_model_batch(params_batch)\n",
    "    \n",
    "    print(f\"     - 已处理批次 {i+1}/{num_batches}\")\n",
    "\n",
    "print(\"   - Sobol 模型运行完成。\")\n",
    "\n",
    "# --- e. 执行分析并保存结果 ---\n",
    "print(\"   - 开始进行 Sobol 分析...\")\n",
    "Si = sobol.analyze(sobol_problem, Y, print_to_console=False, calc_second_order=True)\n",
    "print(\"   - Sobol 分析完成。\")\n",
    "\n",
    "# 保存结果到Excel\n",
    "total_Si, first_Si, second_Si = Si.to_df()\n",
    "with pd.ExcelWriter(sobol_results_excel) as writer:\n",
    "    total_Si.to_excel(writer, sheet_name=\"Total_Order_Sensitivity\")\n",
    "    first_Si.to_excel(writer, sheet_name=\"First_Order_Sensitivity\")\n",
    "    second_Si.to_excel(writer, sheet_name=\"Second_Order_Sensitivity\")\n",
    "print(f\"   - Sobol 敏感性分析结果已保存到: {sobol_results_excel}\")\n",
    "\n",
    "# 绘制并保存图表\n",
    "fig, ax = plt.subplots(1, figsize=(12, 7))\n",
    "total_Si.plot(kind='bar', y='ST', yerr='ST_conf', title='Sobol Total-Order Indices (ST)', ax=ax, capsize=4, legend=False)\n",
    "ax.set_ylabel(\"Total-Order Index (ST)\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(sobol_plot_path, dpi=300)\n",
    "print(f\"   - Sobol 敏感性分析图表已保存到: {sobol_plot_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n✅ 所有任务处理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re \n",
    "\n",
    "# ======================= 参数设置 =======================\n",
    "output_dir = r\"G:\\Remote sensing paper\\RESAMPLE\\new revise\" \n",
    "batch_file_prefix = \"iterations_results_batch_\"\n",
    "batch_file_suffix = \"_of_\" \n",
    "output_variance_excel = os.path.join(output_dir, \"geothermal_potential_variance_summary.xlsx\")\n",
    "\n",
    "# ======================= 主程序 =======================\n",
    "\n",
    "print(\"1. 搜寻并确认批次文件...\")\n",
    "all_files = [f for f in os.listdir(output_dir) if f.startswith(batch_file_prefix) and f.endswith(\".xlsx\")]\n",
    "\n",
    "if not all_files:\n",
    "    print(f\"错误：在目录 '{output_dir}' 中未找到任何以 '{batch_file_prefix}' 开头的批次文件。\")\n",
    "    exit()\n",
    "\n",
    "parsed_batches = []\n",
    "max_batch_num = 0\n",
    "for filename in all_files:\n",
    "    match = re.search(r'_batch_(\\d+)_of_(\\d+)\\.xlsx', filename)\n",
    "    if match:\n",
    "        current_batch = int(match.group(1))\n",
    "        total_batches_in_name = int(match.group(2))\n",
    "        parsed_batches.append((current_batch, total_batches_in_name, filename))\n",
    "        max_batch_num = max(max_batch_num, total_batches_in_name)\n",
    "\n",
    "if not parsed_batches:\n",
    "    print(\"错误：未能从文件名中解析出批次信息。请检查文件名格式是否为 'iterations_results_batch_X_of_Y.xlsx'。\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "parsed_batches.sort(key=lambda x: x[0])\n",
    "expected_batches = list(range(1, max_batch_num + 1))\n",
    "found_batch_numbers = [pb[0] for pb in parsed_batches]\n",
    "\n",
    "if set(expected_batches) != set(found_batch_numbers):\n",
    "    print(f\"警告：检测到批次文件缺失或不连续。预期的批次有 {max_batch_num} 个，但只找到了 {len(found_batch_numbers)} 个。\")\n",
    "    print(f\"缺失批次: {set(expected_batches) - set(found_batch_numbers)}\")\n",
    "\n",
    "print(f\"   - 找到 {len(parsed_batches)} 个批次文件，总计 {max_batch_num} 个批次。\")\n",
    "\n",
    "# 存储每个点所有迭代结果\n",
    "# 键为 (X, Y) 坐标元组，值为包含10000个迭代结果的列表\n",
    "point_iterations = {}\n",
    "\n",
    "# 获取坐标点总数，用于进度显示\n",
    "first_batch_path = os.path.join(output_dir, parsed_batches[0][2])\n",
    "df_first = pd.read_excel(first_batch_path)\n",
    "num_points = len(df_first)\n",
    "print(f\"   - 共 {num_points} 个空间点需要处理。\")\n",
    "\n",
    "print(\"\\n2. 逐批次读取迭代结果并汇集...\")\n",
    "for batch_idx, (current_batch, total_batches_in_name, filename) in enumerate(parsed_batches):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    print(f\"   - 正在读取批次 {current_batch}/{total_batches_in_name}：{filename}\")\n",
    "\n",
    "    try:\n",
    "        df_batch = pd.read_excel(file_path)\n",
    "        # 提取迭代结果列\n",
    "        iteration_cols = [col for col in df_batch.columns if col.startswith('iter_')]\n",
    "\n",
    "        # 遍历 DataFrame 的每一行 (即每个空间点)\n",
    "        for idx, row in df_batch.iterrows():\n",
    "            x, y = row['X'], row['Y']\n",
    "            point_key = (x, y)\n",
    "\n",
    "            # 获取当前批次的迭代结果\n",
    "            current_iter_values = row[iteration_cols].tolist()\n",
    "\n",
    "            # 将当前批次的结果添加到该点的总列表中\n",
    "            if point_key not in point_iterations:\n",
    "                point_iterations[point_key] = []\n",
    "            point_iterations[point_key].extend(current_iter_values)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"警告：读取文件 {filename} 时发生错误: {e}。跳过此文件。\")\n",
    "        continue\n",
    "\n",
    "print(\"   - 所有批次文件读取完成。\")\n",
    "\n",
    "print(\"\\n3. 计算每个点的方差...\")\n",
    "variance_results = []\n",
    "processed_count = 0\n",
    "for point_key, iter_values in point_iterations.items():\n",
    "    if len(iter_values) > 1: # 至少需要两个值才能计算方差\n",
    "        variance = np.var(iter_values)\n",
    "    else:\n",
    "        variance = np.nan # 如果只有一个或零个值，方差为 NaN\n",
    "\n",
    "    variance_results.append({'X': point_key[0], 'Y': point_key[1], 'Variance': variance})\n",
    "    processed_count += 1\n",
    "    if processed_count % (num_points // 10) == 0 or processed_count == num_points:\n",
    "        print(f\"   - 已计算 {processed_count}/{num_points} 个点的方差。\")\n",
    "\n",
    "print(\"   - 方差计算完成。\")\n",
    "\n",
    "print(\"\\n4. 保存方差汇总表...\")\n",
    "df_variance_summary = pd.DataFrame(variance_results)\n",
    "df_variance_summary.to_excel(output_variance_excel, index=False)\n",
    "print(f\"   - 方差汇总表已保存到: {output_variance_excel}\")\n",
    "\n",
    "print(\"\\n✅ 方差计算和保存任务完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
